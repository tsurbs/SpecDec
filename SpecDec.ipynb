{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7baee5cf8ce845a3ae27d6529ac39848": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9adbafff5a7142efa0e677903f6673e6",
              "IPY_MODEL_24df765ff15149b490b02e69fc61ce07",
              "IPY_MODEL_a2b9daf9ce81492fbd998880451d0697"
            ],
            "layout": "IPY_MODEL_593378ed978c4107a167787a885c3abf"
          }
        },
        "9adbafff5a7142efa0e677903f6673e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1339a647e2f64e74b7d7dc306a5b2004",
            "placeholder": "​",
            "style": "IPY_MODEL_45d3f6c7300c42748cecd1c0b128405a",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "24df765ff15149b490b02e69fc61ce07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12db81e8417c4572809dc03bf0b3e78c",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_affb23d653ee4a0b8ae8f374d8859f9f",
            "value": 3
          }
        },
        "a2b9daf9ce81492fbd998880451d0697": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_277f986ea5fd436ba90ae05b155864d7",
            "placeholder": "​",
            "style": "IPY_MODEL_e639d1919f834687a641e6a359819fc6",
            "value": " 3/3 [00:14&lt;00:00,  4.14s/it]"
          }
        },
        "593378ed978c4107a167787a885c3abf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1339a647e2f64e74b7d7dc306a5b2004": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45d3f6c7300c42748cecd1c0b128405a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "12db81e8417c4572809dc03bf0b3e78c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "affb23d653ee4a0b8ae8f374d8859f9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "277f986ea5fd436ba90ae05b155864d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e639d1919f834687a641e6a359819fc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oN1oi-2hJGsl"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 1. Load Tokenizer\n",
        "# checkpoint_verifier = \"gpt2-large\"\n",
        "# checkpoint_draft = \"distilgpt2\"\n",
        "# checkpoint_verifier = \"Qwen/Qwen2.5-7B\"\n",
        "# checkpoint_draft = \"Qwen/Qwen2.5-0.5B\"\n",
        "checkpoint_verifier = \"EleutherAI/pythia-12b\"\n",
        "checkpoint_draft = \"EleutherAI/pythia-70m\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint_verifier)\n",
        "\n",
        "# 2. Load Verifier Model\n",
        "print(f\"Loading verifier model: {checkpoint_verifier}...\")\n",
        "# verifier_model = AutoModelForCausalLM.from_pretrained(checkpoint_verifier).to(device)\n",
        "verifier_model = AutoModelForCausalLM.from_pretrained(\n",
        "    checkpoint_verifier,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# 3. Load Draft Model\n",
        "print(f\"Loading draft model: {checkpoint_draft}...\")\n",
        "# draft_model = AutoModelForCausalLM.from_pretrained(checkpoint_draft).to(device)\n",
        "draft_model = AutoModelForCausalLM.from_pretrained(\n",
        "    checkpoint_draft,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Ensure models are in eval mode\n",
        "verifier_model.eval()\n",
        "draft_model.eval()\n",
        "\n",
        "print(\"Models loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "7baee5cf8ce845a3ae27d6529ac39848",
            "9adbafff5a7142efa0e677903f6673e6",
            "24df765ff15149b490b02e69fc61ce07",
            "a2b9daf9ce81492fbd998880451d0697",
            "593378ed978c4107a167787a885c3abf",
            "1339a647e2f64e74b7d7dc306a5b2004",
            "45d3f6c7300c42748cecd1c0b128405a",
            "12db81e8417c4572809dc03bf0b3e78c",
            "affb23d653ee4a0b8ae8f374d8859f9f",
            "277f986ea5fd436ba90ae05b155864d7",
            "e639d1919f834687a641e6a359819fc6"
          ]
        },
        "id": "RZ_7UVwNJMjO",
        "outputId": "1fb72c37-8c0a-4bf7-db36-4dc31449a382"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading verifier model: EleutherAI/pythia-12b...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7baee5cf8ce845a3ae27d6529ac39848"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading draft model: EleutherAI/pythia-70m...\n",
            "Models loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def standard_autoregressive_generation(model, input_ids, max_new_tokens):\n",
        "    \"\"\"\n",
        "    Generates text using standard autoregressive decoding.\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Generate\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False, # Greedy decoding\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    end_time = time.time()\n",
        "    latency = end_time - start_time\n",
        "    return output, latency"
      ],
      "metadata": {
        "id": "OLJfCp1jJXQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def speculative_decoding(verifier, draft, input_ids, max_new_tokens, gamma=4):\n",
        "    \"\"\"\n",
        "    Implementation of Speculative Decoding.\n",
        "    gamma (k): Number of tokens the draft model guesses at once.\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    total_draft_tokens = 0\n",
        "    accepted_draft_tokens = 0\n",
        "\n",
        "    curr_input_ids = input_ids.clone()\n",
        "\n",
        "    while curr_input_ids.shape[1] < input_ids.shape[1] + max_new_tokens:\n",
        "\n",
        "        # --- Step 1: Draft Model generates gamma (k) tokens ---\n",
        "        draft_outputs = draft.generate(\n",
        "            curr_input_ids,\n",
        "            max_new_tokens=gamma,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        draft_tokens = draft_outputs[0, curr_input_ids.shape[1]:]\n",
        "\n",
        "        # --- Step 2: Verifier checks the draft ---\n",
        "        verifier_input = torch.cat([curr_input_ids, draft_tokens.unsqueeze(0)], dim=1)\n",
        "\n",
        "        verifier_outputs = verifier(verifier_input)\n",
        "        logits = verifier_outputs.logits\n",
        "\n",
        "        start_pos = curr_input_ids.shape[1] - 1\n",
        "        end_pos = verifier_input.shape[1] - 1\n",
        "        predicted_tokens = torch.argmax(logits[0, start_pos:end_pos], dim=-1)\n",
        "\n",
        "        # --- Step 3: Acceptance Loop ---\n",
        "        n_matches = 0\n",
        "        for i in range(len(draft_tokens)):\n",
        "            if draft_tokens[i] == predicted_tokens[i]:\n",
        "                n_matches += 1\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        total_draft_tokens += len(draft_tokens)\n",
        "        accepted_draft_tokens += n_matches\n",
        "\n",
        "        # --- Step 4: Append Accepted Tokens ---\n",
        "        accepted_sequence = draft_tokens[:n_matches]\n",
        "        curr_input_ids = torch.cat([curr_input_ids, accepted_sequence.unsqueeze(0)], dim=1)\n",
        "\n",
        "        # --- Step 5: Correction --\n",
        "        if n_matches < len(draft_tokens):\n",
        "            correction_token = predicted_tokens[n_matches]\n",
        "            curr_input_ids = torch.cat([curr_input_ids, correction_token.unsqueeze(0).unsqueeze(0)], dim=1)\n",
        "\n",
        "        if curr_input_ids.shape[1] >= input_ids.shape[1] + max_new_tokens:\n",
        "            break\n",
        "\n",
        "    end_time = time.time()\n",
        "    latency = end_time - start_time\n",
        "\n",
        "    acceptance_rate = accepted_draft_tokens / total_draft_tokens if total_draft_tokens > 0 else 0\n",
        "\n",
        "    return curr_input_ids, latency, acceptance_rate"
      ],
      "metadata": {
        "id": "ZCl3w06YJerY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def benchmark_speculative_decoding(prompt, max_new_tokens=200, gamma=5):\n",
        "    \"\"\"\n",
        "    Runs and compares standard vs speculative decoding for a given prompt.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Prepare Inputs\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    input_ids = inputs.input_ids\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"PROMPT: {prompt.strip()[:100]}...\" if len(prompt) > 100 else f\"PROMPT: {prompt.strip()}\")\n",
        "    print(f\"Settings: max_new_tokens={max_new_tokens}, gamma={gamma}\")\n",
        "    print(f\"{'-'*80}\")\n",
        "\n",
        "    # 2. Run Standard Decoding (Baseline)\n",
        "    print(\"Running Standard Decoding (Baseline)...\")\n",
        "    baseline_output, baseline_time = standard_autoregressive_generation(\n",
        "        verifier_model, input_ids, max_new_tokens\n",
        "    )\n",
        "    baseline_text = tokenizer.decode(baseline_output[0], skip_special_tokens=True)\n",
        "    print(f\"Baseline Time: {baseline_time:.4f}s\")\n",
        "\n",
        "    # 3. Run Speculative Decoding\n",
        "    print(\"\\nRunning Speculative Decoding...\")\n",
        "    spec_output, spec_time, acc_rate = speculative_decoding(\n",
        "        verifier_model, draft_model, input_ids, max_new_tokens, gamma=gamma\n",
        "    )\n",
        "\n",
        "    # 4. Alignment & Comparison\n",
        "    min_len = min(baseline_output.shape[1], spec_output.shape[1])\n",
        "    baseline_output_trunc = baseline_output[:, :min_len]\n",
        "    spec_output_trunc = spec_output[:, :min_len]\n",
        "\n",
        "    spec_text = tokenizer.decode(spec_output_trunc[0], skip_special_tokens=True)\n",
        "    print(f\"Speculative Time: {spec_time:.4f}s\")\n",
        "    print(f\"Acceptance Rate: {acc_rate:.2%}\")\n",
        "\n",
        "    # 5. Metrics\n",
        "    print(f\"{'-'*50}\")\n",
        "    speedup = baseline_time / spec_time\n",
        "    print(f\"SPEEDUP: {speedup:.2f}x\")\n",
        "    print(f\"{'-'*50}\")\n",
        "\n",
        "    # 6. Validation\n",
        "    if torch.all(baseline_output_trunc == spec_output_trunc):\n",
        "        print(\"SUCCESS: Outputs match exactly!\")\n",
        "    else:\n",
        "        print(\"NOTE: Outputs differ.\")\n",
        "        matches = (baseline_output_trunc == spec_output_trunc).sum().item()\n",
        "        total = baseline_output_trunc.shape[1]\n",
        "        print(f\"Consistency: {matches}/{total} tokens matched ({(matches/total):.1%})\")\n",
        "\n",
        "    # 7. Print Outputs\n",
        "    print(\"\\n--- Baseline Output ---\")\n",
        "    print(baseline_text)\n",
        "    print(\"\\n--- Speculative Output ---\")\n",
        "    print(spec_text)\n",
        "    print(f\"{'='*80}\\n\")"
      ],
      "metadata": {
        "id": "IaT6XLegPEM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GAMMA = 5\n",
        "\n",
        "prompt = \"The quick brown fox jumps over the\"\n",
        "max_new_tokens = 20\n",
        "gamma = GAMMA\n",
        "benchmark_speculative_decoding(prompt, max_new_tokens, gamma)\n",
        "\n",
        "prompt2 = \"\"\"def fibonacci(n):\n",
        "    \\\"\\\"\\\"\n",
        "    Returns the nth number in the fibonacci sequence.\n",
        "    \\\"\\\"\\\"\n",
        "\"\"\"\n",
        "max_new_tokens2 = 100\n",
        "gamma2 = GAMMA\n",
        "benchmark_speculative_decoding(prompt2, max_new_tokens2, gamma2)\n",
        "\n",
        "prompt3 = \"\"\"\n",
        "def two_sum(nums, target):\n",
        "    num_map = {}\n",
        "    for i, num in enumerate(nums):\n",
        "\"\"\"\n",
        "max_new_tokens3 = 100\n",
        "gamma3 = GAMMA\n",
        "benchmark_speculative_decoding(prompt3, max_new_tokens3, gamma3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDoJxU7iJ_hq",
        "outputId": "a4675d33-606e-477b-aa31-e4ebcb9c95f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "PROMPT: The quick brown fox jumps over the\n",
            "Settings: max_new_tokens=20, gamma=5\n",
            "--------------------------------------------------------------------------------\n",
            "Running Standard Decoding (Baseline)...\n",
            "Baseline Time: 69.1623s\n",
            "\n",
            "Running Speculative Decoding...\n",
            "Speculative Time: 42.0315s\n",
            "Acceptance Rate: 20.00%\n",
            "--------------------------------------------------\n",
            "SPEEDUP: 1.65x\n",
            "--------------------------------------------------\n",
            "SUCCESS: Outputs match exactly!\n",
            "\n",
            "--- Baseline Output ---\n",
            "The quick brown fox jumps over the lazy dog\" \"That's all, folks.\" \"Thank you very much.\" \"Thank you.\"\n",
            "\n",
            "--- Speculative Output ---\n",
            "The quick brown fox jumps over the lazy dog\" \"That's all, folks.\" \"Thank you very much.\" \"Thank you.\"\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "PROMPT: def fibonacci(n):\n",
            "    \"\"\"\n",
            "    Returns the nth number in the fibonacci sequence.\n",
            "    \"\"\"\n",
            "Settings: max_new_tokens=100, gamma=5\n",
            "--------------------------------------------------------------------------------\n",
            "Running Standard Decoding (Baseline)...\n",
            "Baseline Time: 343.0791s\n",
            "\n",
            "Running Speculative Decoding...\n",
            "Speculative Time: 114.6117s\n",
            "Acceptance Rate: 46.25%\n",
            "--------------------------------------------------\n",
            "SPEEDUP: 2.99x\n",
            "--------------------------------------------------\n",
            "SUCCESS: Outputs match exactly!\n",
            "\n",
            "--- Baseline Output ---\n",
            "def fibonacci(n):\n",
            "    \"\"\"\n",
            "    Returns the nth number in the fibonacci sequence.\n",
            "    \"\"\"\n",
            "    if n == 0:\n",
            "        return 0\n",
            "    elif n == 1:\n",
            "        return 1\n",
            "    else:\n",
            "        return fibonacci(n-1) + fibonacci(n-2)\n",
            "\n",
            "print(fibonacci(10))\n",
            "\n",
            "A:\n",
            "\n",
            "You can use the built-in function range to generate the numbers.\n",
            ">>> def fibonacci(n):\n",
            "...     return sum(range(1, n+1))\n",
            "...\n",
            "\n",
            "--- Speculative Output ---\n",
            "def fibonacci(n):\n",
            "    \"\"\"\n",
            "    Returns the nth number in the fibonacci sequence.\n",
            "    \"\"\"\n",
            "    if n == 0:\n",
            "        return 0\n",
            "    elif n == 1:\n",
            "        return 1\n",
            "    else:\n",
            "        return fibonacci(n-1) + fibonacci(n-2)\n",
            "\n",
            "print(fibonacci(10))\n",
            "\n",
            "A:\n",
            "\n",
            "You can use the built-in function range to generate the numbers.\n",
            ">>> def fibonacci(n):\n",
            "...     return sum(range(1, n+1))\n",
            "...\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "PROMPT: def two_sum(nums, target):\n",
            "    num_map = {}\n",
            "    for i, num in enumerate(nums):\n",
            "Settings: max_new_tokens=100, gamma=5\n",
            "--------------------------------------------------------------------------------\n",
            "Running Standard Decoding (Baseline)...\n",
            "Baseline Time: 345.2236s\n",
            "\n",
            "Running Speculative Decoding...\n",
            "Speculative Time: 133.4390s\n",
            "Acceptance Rate: 38.38%\n",
            "--------------------------------------------------\n",
            "SPEEDUP: 2.59x\n",
            "--------------------------------------------------\n",
            "SUCCESS: Outputs match exactly!\n",
            "\n",
            "--- Baseline Output ---\n",
            "\n",
            "def two_sum(nums, target):\n",
            "    num_map = {}\n",
            "    for i, num in enumerate(nums):\n",
            "        if num in num_map:\n",
            "            num_map[num] += 1\n",
            "        else:\n",
            "            num_map[num] = 1\n",
            "    for num in num_map:\n",
            "        if num == target:\n",
            "            return num_map.keys()\n",
            "    return []\n",
            "\n",
            "print(two_sum([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 6))\n",
            "\n",
            "A:\n",
            "\n",
            "You can use\n",
            "\n",
            "--- Speculative Output ---\n",
            "\n",
            "def two_sum(nums, target):\n",
            "    num_map = {}\n",
            "    for i, num in enumerate(nums):\n",
            "        if num in num_map:\n",
            "            num_map[num] += 1\n",
            "        else:\n",
            "            num_map[num] = 1\n",
            "    for num in num_map:\n",
            "        if num == target:\n",
            "            return num_map.keys()\n",
            "    return []\n",
            "\n",
            "print(two_sum([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 6))\n",
            "\n",
            "A:\n",
            "\n",
            "You can use\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}