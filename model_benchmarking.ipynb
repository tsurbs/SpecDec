{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30d05044",
   "metadata": {},
   "source": [
    "# Speculative Decoding Baseline Testing\n",
    "\n",
    "This notebook provides an interactive interface for running speculative decoding benchmarks across multiple model configurations and datasets.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, ensure all dependencies are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfceef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "!pip install torch transformers accelerate datasets numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8e8679",
   "metadata": {},
   "source": [
    "## Sync Files to Colab (Choose One Method)\n",
    "\n",
    "If you're running this notebook in Google Colab, you need to sync the `baseline_test_utils.py.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11b45ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sync baseline_test_utils.py.py (for Colab users)\n",
    "import os\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB and not os.path.exists('baseline_test_utils.py.py'):\n",
    "    import urllib.request\n",
    "    github_url = \"https://raw.githubusercontent.com/tsurbs/SpecDec/main/baseline_test_utils.py.py\"\n",
    "    try:\n",
    "        urllib.request.urlretrieve(github_url, 'baseline_test_utils.py.py')\n",
    "        print(\"Downloaded baseline_test_utils.py.py from GitHub\")\n",
    "    except Exception as e:\n",
    "        print(f\"Download failed: {e}\")\n",
    "        print(\"Manually upload the file via Colab's file browser\")\n",
    "elif not IN_COLAB:\n",
    "    print(\"Using local files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb81a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from baseline_test_utils.py import (\n",
    "    SpeculativeDecodingTester,\n",
    ")\n",
    "\n",
    "from load_datasets import (\n",
    "    load_pile_samples,\n",
    "    load_stack_samples\n",
    ")\n",
    "\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2dae40",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dedf1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Configurations\n",
    "MODEL_CONFIGS = {\n",
    "    'GPT-2': {\n",
    "        'verifier': 'gpt2-large',\n",
    "        'draft': 'distilgpt2'\n",
    "    },\n",
    "    'Qwen': {\n",
    "        'verifier': 'Qwen/Qwen2.5-7B',\n",
    "        'draft': 'Qwen/Qwen2.5-0.5B'\n",
    "    },\n",
    "    'Pythia': {\n",
    "        'verifier': 'EleutherAI/pythia-12b',\n",
    "        'draft': 'EleutherAI/pythia-70m'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Test Parameters\n",
    "TEST_PARAMS = {\n",
    "    'max_new_tokens': 100,\n",
    "    'gamma': 5,\n",
    "    'num_nl_samples': 100,\n",
    "    'num_code_samples': 100\n",
    "}\n",
    "\n",
    "CODE_LANGUAGES = ['python', 'c', 'go', 'rust']\n",
    "\n",
    "print(f\"Models: {list(MODEL_CONFIGS.keys())}, Gamma: {TEST_PARAMS['gamma']}, Languages: {CODE_LANGUAGES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6fba52",
   "metadata": {},
   "source": [
    "## Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4b7c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Natural Language samples from The Pile\n",
    "nl_prompts = load_pile_samples(TEST_PARAMS['num_nl_samples'])\n",
    "print(f\"Loaded {len(nl_prompts)} NL samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35cc5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Code samples from The Stack\n",
    "code_prompts = load_stack_samples(CODE_LANGUAGES, TEST_PARAMS['num_code_samples'])\n",
    "print(f\"Loaded {len(code_prompts)} code samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d42dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all prompts\n",
    "all_prompts = nl_prompts + code_prompts\n",
    "print(f\"Total: {len(all_prompts)} prompts ({len(nl_prompts)} NL + {len(code_prompts)} Code)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02468fc9",
   "metadata": {},
   "source": [
    "## Test on Individual Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8335a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model to test\n",
    "MODEL_TO_TEST = 'GPT-2'  # Options: 'GPT-2', 'Qwen', 'Pythia'\n",
    "\n",
    "config = MODEL_CONFIGS[MODEL_TO_TEST]\n",
    "print(f\"Testing {MODEL_TO_TEST}: {config['verifier']} + {config['draft']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff23d52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tester\n",
    "tester = SpeculativeDecodingTester(\n",
    "    verifier_checkpoint=config['verifier'],\n",
    "    draft_checkpoint=config['draft']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a45876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run quick validation test\n",
    "result = tester.run_single_test(\n",
    "    prompt=all_prompts[0]['text'],\n",
    "    max_new_tokens=50,\n",
    "    gamma=TEST_PARAMS['gamma'],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8d5baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run full benchmark on selected model\n",
    "print(f\"Running full benchmark on {MODEL_TO_TEST}...\")\n",
    "results = tester.run_benchmark_suite(\n",
    "    prompts=all_prompts,\n",
    "    max_new_tokens=TEST_PARAMS['max_new_tokens'],\n",
    "    gamma=TEST_PARAMS['gamma']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9a7c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results for this model\n",
    "output_file = f\"results_{MODEL_TO_TEST.lower()}.json\"\n",
    "\n",
    "def make_serializable(obj):\n",
    "    import numpy as np\n",
    "    if isinstance(obj, (np.floating, np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.integer, np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: make_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [make_serializable(i) for i in obj]\n",
    "    return obj\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(make_serializable(results), f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e1d4d6",
   "metadata": {},
   "source": [
    "## Run All Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcfb384",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}\n",
    "\n",
    "for model_name, config in MODEL_CONFIGS.items():\n",
    "    print(f\"Testing {model_name}: {config['verifier']} + {config['draft']}\")\n",
    "    \n",
    "    try:\n",
    "        tester = SpeculativeDecodingTester(\n",
    "            verifier_checkpoint=config['verifier'],\n",
    "            draft_checkpoint=config['draft']\n",
    "        )\n",
    "        \n",
    "        results = tester.run_benchmark_suite(\n",
    "            prompts=all_prompts,\n",
    "            max_new_tokens=TEST_PARAMS['max_new_tokens'],\n",
    "            gamma=TEST_PARAMS['gamma']\n",
    "        )\n",
    "        \n",
    "        all_results[model_name] = results\n",
    "        \n",
    "        # Print summary\n",
    "        for ptype, metrics in results['summary'].items():\n",
    "            print(f\"{ptype}: Acc={metrics['avg_acceptance_rate']}, Speedup={metrics['avg_speedup']}x\")\n",
    "        \n",
    "        # Clean up\n",
    "        del tester\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        print(f\"{model_name} complete\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"All tests complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9253e595",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ad71e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results\n",
    "with open(\"baseline_results_all.json\", 'w') as f:\n",
    "    json.dump(make_serializable(all_results), f, indent=2)\n",
    "print(\"Results saved to baseline_results_all.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
