\documentclass[twocolumn,11pt]{article}

% Packages
\usepackage{natbib}      % For bibliography management
\usepackage{graphicx}    % For including graphics
\usepackage{amsmath}     % For math symbols and equations
\usepackage{amssymb}     % mathbb
\usepackage{geometry}    % To adjust page margins
\usepackage{hyperref}    % For hyperlinks in the document
\usepackage{times}
\usepackage[switch]{lineno}      % For line numbers
\usepackage{float}
% Set the page geometry
\geometry{a4paper, margin=0.75in}

% Begin Document
\begin{document}

% Title
\title{SpecDec}
\author{Avi Arya \& Shreya Singh \& Theo Urban\\
\texttt{avi1@cmu.edu, shreyasi@cs.cmu.edu, tsurban@cs.cmu.edu}\\
10-423/623 Generative AI Course Project}
\date{\today}
\maketitle

\linenumbers

\section{Introduction}
%  Concise overview of the entire executive summary. The introduction
% should in length be similar to that of a long abstract. It should highlight your motivation, proposed method, and expected results.
As large language models have continued to grow, inference latency has become a bigger issue. Improvements in inference latency can significantly affect workflows, and having lower latency may result in faster systems and feedback loops. A recent proposal to address the issue of inference latency is Speculative Decoding, a method of using smaller models to generate a response, then using a larger language model to attempt to either accept or disregard the smaller language model's result. If the larger language model does not accept the smaller language model's result, then the larger language model generates its own result fully. However, the hope is that, oftentimes, the smaller language model and the larger language model will result, resulting in improved latency.


In this project, we hope to apply finetuning to Speculative Decoding to create a faster model for code generation. Specifically, we will use The Stack, a dataset of code files, to finetune a drafter model, then use this as the small model in our speculative decoding model, eventually comparing this model to speculative decoding without this additional finetuning. 


\section{Dataset / Task}
% Detailed description of the task, dataset, and metric(s) for evaluation.
% This section should be nearly in its final form.
For this project, we utilize The Stack, a large-scale dataset of source code developed by the BigCode project. While the full dataset contains over 6 terabytes of data, utilizing the entire corpus is computationally infeasible for our training resources. Consequently, we are curating a specific fine-tuning subset described below.

\subsection{Dataset Curation for Fine-tuning}
Our fine-tuning strategy focuses on domain adaptation for the \texttt{pythia-70m} draft model. Rather than training on the entire Stack, we are creating a targeted training split consisting of approximately 5,000 to 10,000 examples per language. 

We selected a mix of high-resource languages (Python, C) and lower-resource languages (Go, Rust) to evaluate cross-lingual generalization. The data is filtered to ensure we are training on valid function bodies and documentation blocks, maximizing the signal for code completion tasks. The test set comprises held-out data from these same languages to ensure a fair evaluation of the \textit{fine-tuned} draft model ($Q'_\phi$) against the \textit{baseline} draft model ($Q_\phi$).

\subsection{Task Definition}
The primary task is domain-specific accelerated code completion. Given an input context $s$ (a string of source code), the goal is to generate a syntactically correct continuation using a speculative decoding framework. The success of this task is measured by the ability of a small \textit{draft model} to predict tokens that are accepted by a large \textit{verifier model}, thereby reducing overall inference latency.

We define our two primary models as follows:
\begin{enumerate}
    \item \textbf{Verifier Model ($P_\theta$)} represents the probability distribution $p(x\mid s)$ over the next token $x$ given some context $s$. For this project, we will use the Pythia-12B model as $P_\theta$.
    \item \textbf{Draft Model ($Q_\phi$)} represents the probability distribution $q(x \mid s)$. For this project, we will use Pythia-70M which is significantly smaller and faster than the verifier model.
\end{enumerate}

Our project will compare two distinct draft models:
\begin{enumerate}
    \item \textbf{Baseline Draft Model ($Q_\phi$)} which is the unmodified Pythia-70M model.
    \item \textbf{Finetuned Draft Model ($Q'_\phi$)} which is the Pythia-70M model after it has been finetuned on the training split from The Stack.
\end{enumerate}

\subsection{Progress: Benchmarking \& Model Selection}
In our initial proposal, we aimed to utilize minGPT and GPT-2. However, to ensure a robust evaluation, we expanded our scope to implement a benchmarking suite that evaluates three distinct model families. This allowed us to assess how different architectures and size ratios affect baseline speculative performance before committing to a fine-tuning candidate.

We evaluated the following pairings on both Natural Language (The Pile) and Code (The Stack) prompts:
\begin{enumerate}
    \item \textbf{GPT-2 Family}: Verifier (\texttt{gpt2-large}) / Draft (\texttt{distilgpt2}).
    \item \textbf{Qwen 2.5 Family}: Verifier (\texttt{Qwen2.5-7B}) / Draft (\texttt{Qwen2.5-0.5B}).
    \item \textbf{Pythia Family}: Verifier (\texttt{pythia-12b}) / Draft (\texttt{pythia-70m}).
\end{enumerate}

We conducted baseline tests across four programming languages—Python, C, Go, and Rust—using a fixed draft length of $\gamma=5$. 

Based on these experiments, we have selected the Pythia family as our primary testbed. We selected this pairing for two reasons:
\begin{enumerate}
    \item \textbf{Extreme Size Ratio ($171:1$)}: The draft model (70M) is significantly smaller than the verifier (12B), offering the highest theoretical ceiling for speedups if acceptance rates can be improved.
    \item \textbf{Scientific Isolation}: Unlike Qwen, which is already highly optimized for code, Pythia is a standard research model. This ensures that any performance gains we observe are attributable to our fine-tuning, rather than the base model's pre-existing coding capabilities.
\end{enumerate}

\subsection{Metrics}
There are three key metrics we can look at to judge the efficacy and robustness of our approach.  1. Acceptance Rate, 2. Speedup over baseline, and 3. differences across languages.  

\begin{enumerate}
    \item \textbf{Acceptance rate}: Acceptance rate is a straightforward metric measuring the ratio of accepted to total tokens generated by the small model. For a single step, this can be expressed as 
    \[\alpha(s) = \sum_{x\in\mathcal{V}}\min(p(x|s), q(x|s))\]
    Where p is our verifier distribution and q is our draft distribution. For typical general purpose speculative decoding models, this Acceptance Rate is between 20-60\%, depending on a variety of factors including language \citet{sandler2025disparateimpactsspeculativedecoding}.  For our purposes, we will compare the acceptance rate of completions by the non-finetuned minGPT model on our code dataset to the acceptance rate after finetuning the draft model.
    \item \textbf{Speedup}: Speedup should be directly correlated with the acceptance rate since the slow step is dominated by un-accepted drafts.  However, it is still a useful metric to observe the real-world potential of any improvements in acceptance rate. This should define our speedup: \[\mathrm{Speedup}(s;\,\gamma,c)= \frac{1 - \alpha(s)^{\gamma + 1}}{(1 - \alpha(s))\, [\gamma c + 1]},\] where $\gamma$ is how many tokens are drafted in parallel and $c=\frac{\text{Drafter pass time}}{\text{Verifier pass time}}$ as defined in \citet{sandler2025disparateimpactsspeculativedecoding}.  $c$ and the optimal value of $\gamma$ are dependent on the specific compute we are using.
    \item \textbf{Language Differences}: Since speculative decoding is faster when it is predicting higher likelihood sequences, it can struggle on lower-resource languages.  Since our dataset is multi-lingual, we have an opportunity to examine the impacts of our fine-tuning on non-english language performance.  Following \citet{sandler2025disparateimpactsspeculativedecoding}, we will measure this disparity across languages in our dataset with
    \[U(\mathcal{T}) = \frac{1}{m} \sum_{T \in \mathcal{T}} (D_T - \min_{T \in \mathcal{T}} D_T)^2\], 
    where $D_T$ is the task-wise cross entropy \[D_T = \mathbb{E}_{s \sim T}\!\left[ - \sum_x p(x \mid s)\, \log q(x \mid s) \right].\]

\end{enumerate}

\section{Related Work}
%  Related Work. A short literature survey of 8 or more relevant papers. This section
% should be nearly in its final form.

While speculative decoding provides a theoretical framework for lossless acceleration, its practical efficacy is bounded by the alignment between the draft and verifier distributions. Our project, thus, aims to improve inference efficiency algorithmically while prioritizing domain-specific language modeling.  

\textbf{The Memory Bottleneck.}
LLM inference is fundamentally memory-bandwidth bound rather than compute bound. \citet{leviathan2023fastinferencetransformersspeculative} and \citet{chen2023acceleratinglargelanguagemodel} formalized Speculative Decoding to address this by converting sequential memory access into parallel compute. By employing a "draft-then-verify" paradigm, they trade excess arithmetic capacity for reduced memory reads. Crucially, they proved that rejection sampling ensures the target distribution is preserved exactly. However, these foundational works primarily evaluate general-domain text. Our project interrogates whether these efficiency gains hold in the high-entropy, strictly structured domain of code generation, where a single token mismatch (e.g., an incorrect indentation or bracket) can derail an entire speculative sequence.

\textbf{The Alignment-Latency Trade-off.}
The core limitation of speculative decoding is the acceptance rate ($\alpha$). \citet{zhou2024distillspecimprovingspeculativedecoding} demonstrated through DistillSpec that generic small models are often poor drafters because they do not mimic the verifier's idiosyncratic probability distribution. They proposed knowledge distillation to force the drafter to approximate the verifier's logits. Similarly, \citet{goel2024directalignmentdraftmodel} showed that when a verifier is fine-tuned (e.g., for chat), a non-aligned drafter causes speedups to collapse.
Our approach diverges from these methods. Rather than performing expensive logit-distillation from the verifier (Teacher-Student), we posit that domain alignment via Semisupervised Fine-Tuning on high-quality code data (\textit{The Stack}) serves as a computationally efficient proxy for verifier alignment. We test if aligning the draft model to the \textit{ground truth} of code syntax creates sufficient distributional overlap to recover speedups.

\textbf{Small Model Capacity in Code.}
A skepticism regarding our architecture choice (Pythia-70M drafting Pythia-12B) is whether a 70M parameter model possesses sufficient reasoning capabilities to draft valid code. \citet{li2023starcodersourceyou} and \citet{rozière2024codellamaopenfoundation} established that code modeling requires massive scale and specific objectives like infilling. However, \citet{gunasekar2023textbooksneed} challenged the scaling laws with phi-1, demonstrating that small models can achieve state-of-the-art performance if trained on "textbook quality" data. We extend this hypothesis to the extreme: if data quality trumps parameter count, a 70M parameter model fine-tuned on curated data from \textit{The Stack} should capture enough local syntactic patterns to serve as an effective speculative drafter, even if it lacks long-horizon reasoning.

\textbf{Distributional Disparities.}
Acceleration must not come at the cost of equity. \citet{sandler2025disparateimpactsspeculativedecoding} identified a critical failure mode in speculative decoding: "rich" tasks (high-resource languages) enjoy massive speedups, while "poor" tasks (low-resource languages) see little to no gain due to poor drafter alignment. By evaluating our model on a split of high-resource (Python, C++) vs. lower-resource languages (Rust, Go), we explicitly test if domain-specific fine-tuning exacerbates or mitigates these inference disparities.

\textbf{Architectural Alternatives.}
Finally, we acknowledge that auxiliary drafters are not the only path to acceleration. \citet{cai2024medusasimplellminference} (Medusa) and \citet{fu2024breaksequentialdependencyllm} (Lookahead Decoding) propose modifying the verification head or using Jacobi iteration to eliminate the separate draft model entirely. While these methods avoid the memory overhead of a second model, they require invasive architectural changes or complex attention mask manipulations. We retain the two-model speculative approach for its modularity, allowing the verifier to remain a black-box standard transformer while we iterate solely on the specialized drafter.

\section{Approach}
% Approach. Description of both (a) your baseline approach and (b) the main methods that you will implement. This section should be nearly in its final form.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{specdec_diagram.png}
    \caption{Diagram illustrating speculative decoding.}
    \label{fig:specdec_diagram}
\end{figure}

In this project, our overall goal is to determine the impact of finetuning on speculative decoding models. We first started by implementing a speculative decoding pipeline which allowed us to evaluate a few different models in our baseline testing. We selected three different model families (from which we selected a drafter model and a verifier model): Qwen, GPT-2, and Pythia.  We ultimately selected Pythia (specifically, pythia-12b and pythia-70m) for use in our finetuning tasks. 

Our next step is to finetune the smaller pythia-70m model on a subset of data from The Stack. We will careful curate this dataset, selecting subsets of data from a variety of different languages, and finally using this data to finetune our verifier model. 

Finally, we will run experiments to evaluate our finetuned drafter model and to evaluate its speedup and acceptance rate. We will run experiments to determine our model's speedup on both in-domain and out-of-domain (general natural language) tasks. We will also run experiments on different languages to determine how the amount of data in a given language (for the finetuning dataset) impacts the model performance on lower frequency languages. We will lastly also perform experiments to determine an optimal value of $\gamma$, the draft length, on speedup and acceptance rate. 
\section{Experiments}
% Experiments. Precise description of the experiments you will run, and any results if applicable. You are required to present results of a baseline model on your task of interest. You must include skeleton tables/plots—these can be empty at this point. You should also include prose with references to the skeleton tables/plots describing what they will contain.
\subsection{Baseline Performance}
Our first experiment establishes the baseline performance of a few different models and on different tasks - natural language completions and a few coding tasks in different languages outlined in the appendix.  

% Need to convert from fib to averaging strategy with completions from The Stack on different languages
\begin{figure}[H]
\begin{tabular}{l|l|c|c}
Model & Completion & Acce. Rate & Speedup \\
\hline
Qwen & NL & 38.3\% & 0.60x \\
 & Code (c) & 67.9\% & 0.50x \\
 & Code (go) & 67.0\% & 0.58x \\
 & Code (python) & 69.4\% & 0.63x \\
 & Code (rust) & 69.3\% & 0.58x \\
\hline
GPT-2  & NL & 36.5\% & 1.15x \\
 & Code (c) & 63.6\% & 1.62x \\
 & Code (go) & 55.4\% & 1.51x \\
 & Code (python) & 65.0\% & 1.64x \\
 & Code (rust) & 57.2\% & 1.50x \\
\hline
Pythia & NL & 34.7\% & 1.26x \\
 & Code (c) & 54.9\% & 1.17x \\
 & Code (go) & 54.1\% & 1.18x \\
 & Code (python) & 55.0\% & 1.23x \\
 & Code (rust) & 56.2\% & 1.26x \\
\end{tabular}
\caption{Baseline speculative decoding performance across different model pairs and tasks. Qwen: (Qwen2.5-7B+Qwen2.5-0.5B), GPT-2: (gpt2-large+distilgpt2), Pythia: (pythia-12b+pythia-70m).}
\end{figure}

The main purpose of this experiment was to select a good model pair for our finetuning experiments.  We ultimately selected Pythia due to its strong speedup performance and reasonable acceptance rates across both natural language and code tasks.  It's also worth it to explain the less-than-one speedup results for Qwen.  We hypthesize that this is due to the fact that the smaller model is still quite large (0.5B parameters) and thus does not provide a significant speed advantage over the larger model (8B parameters).  We confirmed this by comparing to performance with a larger Qwen verifier, where we did observe a higher speedup.

Next, we outline the proposed experiments we designed to evaluate how domain-specific finetuning of the draft model influences speculative decoding effectiveness, following three research questions.

\subsection{RQ1: Does Domain-Specific Finetuning Improve Speculative Decoding Performance?}
We compare Baseline SpecDec (Pythia verifier + unfinetuned Pythia draft) against Finetuned SpecDec (Pythia verifier + Pythia finetuned on The Stack). Both models are evaluated on in-domain (code completions across Python, C++, Go, Rust) and out-of-domain (natural language) tasks. Each prompt generates 128-token continuations with fixed draft length $\gamma$.

\begin{table}[h]
    \centering
    \begin{tabular}{c|c|c|c|c}
        Model Pair & Domain & Accept Rate & Speedup \\
        \hline
        Baseline & Code (Py) & ??? & ???\\
        Finetuned & Code (Py) & ??? & ???\\
        Baseline & NL & ??? & ???\\
        Finetuned & NL & ??? & ???\\
    \end{tabular}
    \caption{Effect of draft model finetuning on acceptance rate and decoding speed.}
    \label{tab:rq1}
\end{table}

\textbf{Expected Outcome.}  
Finetuning should increase acceptance rate and speedup on code tasks, while maintaining comparable performance on natural language prompts.

\subsection{RQ2: Does Finetuning Improve Performance for Low-Resource Languages?}

\textbf{Goal.}  
Determine whether finetuning disproportionately helps high-resource languages or narrows cross-lingual disparities.

\textbf{Experimental Setup.}  
We construct a language-oriented test set from The Stack with three buckets: (1) \textbf{High}: Python, C++, Java, (2) \textbf{Medium}: Lua, Haskell, Ruby, and (3) \textbf{Low}: Zig, Elixir, OCaml

For each language bucket, we measure (1) acceptance rate, (2) end-to-end speedup, and (3) disparity metric $U(T)$ from \citet{sandler2025disparateimpactsspeculativedecoding} before and after finetuning.

\begin{table}[h]
    \centering
    \begin{tabular}{c|c|c|c}
        Group & Base $U(T)$ & Tuned $U(T)$ & $\Delta$ Disparity \\
        \hline
        High & ??? & ??? & ??? \\
        Medium & ??? & ??? & ??? \\
        Low & ??? & ??? & ??? \\
    \end{tabular}
    \caption{Change in cross-lingual performance disparity after finetuning.}
    \label{tab:rq2}
\end{table}

\textbf{Expected Outcome.}  
Finetuning should reduce performance disparity across languages by improving draft alignment with code structure.

\subsection{RQ3: How Does Draft Length $\gamma$ Affect Speedup Under Fixed Compute?}

\textbf{Goal.}  
Characterize how varying $\gamma$ impacts acceptance rate and overall speedup.

\textbf{Experimental Setup.}  
Using the finetuned draft model, we vary $\gamma \in \{1, 2, 4, 8, 16, 32\}$
For each $\gamma$, we record (1) Acceptance rate and (2) real-time decoding speedup.
% Reduce inter-column spacing and use a smaller font for the upcoming table.
\setlength{\tabcolsep}{3pt} % default is ~6pt
\renewcommand{\arraystretch}{0.9}
\small

% Short, stacked column headings to collapse header width

\begin{table}[h]
    \centering
    \begin{tabular}{c|c|c}
        $\gamma$ & Speedup & Accept Rate \\
        \hline
        1 & ??? & ??? \\
        4 & ??? & ??? \\
        16 & ??? & ??? \\
    \end{tabular}
    \caption{Effect of draft length on rejection patterns and speedup.}
    \label{tab:rq3}
\end{table}


\textbf{Expected Outcome.}  
We hypothesize that moderate values of $\gamma$ maximize speedup by avoiding excessive rejections while still leveraging draft efficiency.

\section{Compute}
% Check piazza
Since Colab recently released Colab Pro to students for free, we have been using A100s (80GB memory) via that. Colab Pro provides 100 compute units for free, which cost \$9.99. However, if we go by pricing for the A100 80GBs, they cost around \$5 per hour and we'll likely need several hours of training time to finetune. Given an additional \$450, we would likely more experiments and try different degrees and types of finetuning. Instead of just training one additional model on a subset of the Stack, we might consider training multiple models on different subsets, or different amounts of training, or try different training tasks. Since we have a limited amount of compute right now, we're trying to prioritize our usage and thus are saving any additional experimentation till the end if we have compute leftover.

% Conclusion
\section{Plan}
% Section 6: Plan. Timeline of remaining milestones with dates and who is responsible for each milestone.
So far, we've set up our initial speculative decoding pipeline with a few different models, including minGPT, GPT2, various sizes of Qwen, and Pythia, and evaluated performance on each of these models. As an initial step, our primary metrics are acceptance rate, speedup, and disparity metric $U(T)$, but if we have sufficient time closer to the deadline, we may experiment with more advanced metrics. As an additional stretch goal, we may also attempt to experiment with dynamically modifying hyperparameters, such as $\gamma$, the draft length, based on the probability of the drafter model's output. 

We have structured the remaining work to align with the course deliverables, specifically the Final Poster (Dec 7) and Final Executive Summary (Dec 11).

\begin{table}[H]
\centering
\small
\begin{tabular}{l|p{4cm}}
\textbf{Dates} & \textbf{Milestone} \\
\hline
Nov 25 - Nov 30 & \textbf{Implementation Sprint}: Finalize finetuning of Pythia-70M ($Q'_\phi$) and integrate into the SpecDec loop.\\
\hline
Dec 1 - Dec 6 & \textbf{Benchmarking \& Poster}: Run full evaluation suite (RQ1, RQ2, RQ3). Compile results into Final Poster PDF. \\
\hline
Dec 8 - Dec 9 & \textbf{Poster Presentation}: Final prep for in-person presentation (Dec 9, 8:30am). \\
\hline
Dec 10 - Dec 11 & \textbf{Final Report}: Synthesize analysis, write limitations/conclusion, and submit Final Executive Summary (Due Dec 11).\\
\end{tabular}
\caption{Project Timeline and Responsibilities.}
\end{table}

\textbf{Roles:}
\begin{itemize}
    \item \textbf{Avi (ML Ops):} Responsible for the training pipeline and ensuring the finetuning setup is ready to be used by all
    \item \textbf{Theo (Systems):} Responsible for the speculative inference loop and measuring end-to-end latency/speedup.
    \item \textbf{Shreya (Evaluation):} Responsible for generating the visual assets for the poster and calculating the disparity metrics $U(T)$ for the final report.
\end{itemize}

% References
\bibliographystyle{plainnat}  % Choose a bibliography style (e.g., plainnat, abbrvnat)
\bibliography{references}     % Add your .bib file here

\appendix

\section{Example Appendix}
\label{sec:appendix}

\subsection{Example Prompts}
{\small
\textbf{Natural Language}
\begin{verbatim}
Prompt: The history of artificial 
intelligence began in antiquity, 
with myths, stories and rumors...

Prompt: Climate change is one of 
the most pressing issues facing 
humanity today...

Prompt: In the realm of quantum 
mechanics, particles exhibit 
behaviors that seem paradoxical...
\end{verbatim}
\textbf{Code (Python)}
\begin{verbatim}
Prompt: if fingerprint_comparison
.nlu == True: ...

Prompt: options['filetype'] = 
'denite'
if self._vim.call('exists', 
'#WinEnter'): ...

Prompt: def _stdchannel_redirected
(stdchannel, dest_filename, 
mode="w"):
    """A context manager..."""
\end{verbatim}
\textbf{Code (C)}
\begin{verbatim}
Prompt: struct vlist_tree;
struct vlist_node;
typedef void (*vlist_update_cb)
(struct vlist_tree *tree, ...

Prompt: return True if HTSFile 
is closed.
return self.htsfile == NULL

Prompt: int isStrL = 0;
if(strstr(calcPath, 
"strategyL/")!=NULL) {
    isStrL = 1; } ...
\end{verbatim}

\textbf{Code (Go)}
\begin{verbatim}
Prompt: func (ip *IPv6) 
pseudoheaderChecksum() 
(csum uint32, err error) {...

Prompt: result, metadata, err := 
c.invokeOperation(ctx, 
"UpdateGameServer", params, ...

Prompt: size := 100
lg := NewLog(log.Size(size))
// make sure we have the r...
\end{verbatim}

\textbf{Code (Rust)}
\begin{verbatim}
Prompt: impl From<Cow<'static, 
[u8]>> for Entry {
    fn from(content: 
    Cow<'static, [u8]>) ...

Prompt: fn resolve(&mut self, 
piet_text: &mut PietText, 
data: &SearchResult) {...

Prompt: None => Path::new("."),
pub fn change_dir_and_return_
parent(filename: &Path) -> ...
\end{verbatim}
}

\end{document}